{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21330d8",
   "metadata": {},
   "source": [
    "## Sentiment Classification AU Reviews Data (BOW, non-Deep Learning)\n",
    "\n",
    "This notebook covers two good approaches to perform sentiment classification - Naive Bayes and Logistic Regression. We will train AU reviews data on both.\n",
    "\n",
    "As a rule of thumb, reviews that are 3 stars and above are **positive**, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a09b873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /home/meln/.local/lib/python3.8/site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/meln/.local/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926204c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import spacy\n",
    "import nltk\n",
    "import gensim.downloader\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.language import Language\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22215586",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1fcb6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>app</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>Iâ€™ve been a DoorDash user for a while now and ...</td>\n",
       "      <td>3</td>\n",
       "      <td>DoorDash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>I ordered a meal for delivery and after 1:30 I...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-03</td>\n",
       "      <td>I have gotten three orders from Doordash, all ...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>The delay and customer support I experienced w...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>I have had countless problems using DoorDash s...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                             review  rating  \\\n",
       "0  2020-07-11  Iâ€™ve been a DoorDash user for a while now and ...       3   \n",
       "1  2020-05-26  I ordered a meal for delivery and after 1:30 I...       1   \n",
       "2  2020-09-03  I have gotten three orders from Doordash, all ...       1   \n",
       "3  2021-08-13  The delay and customer support I experienced w...       1   \n",
       "4  2021-11-01  I have had countless problems using DoorDash s...       1   \n",
       "\n",
       "        app  \n",
       "0  DoorDash  \n",
       "1  DoorDash  \n",
       "2  DoorDash  \n",
       "3  DoorDash  \n",
       "4  DoorDash  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_pickle(\"assets/au_reviews.pkl\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49be95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['label'] = np.where(reviews['rating'] >= 3, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "570595bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>app</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>Iâ€™ve been a DoorDash user for a while now and ...</td>\n",
       "      <td>3</td>\n",
       "      <td>DoorDash</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>I ordered a meal for delivery and after 1:30 I...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-03</td>\n",
       "      <td>I have gotten three orders from Doordash, all ...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>The delay and customer support I experienced w...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>I have had countless problems using DoorDash s...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                             review  rating  \\\n",
       "0  2020-07-11  Iâ€™ve been a DoorDash user for a while now and ...       3   \n",
       "1  2020-05-26  I ordered a meal for delivery and after 1:30 I...       1   \n",
       "2  2020-09-03  I have gotten three orders from Doordash, all ...       1   \n",
       "3  2021-08-13  The delay and customer support I experienced w...       1   \n",
       "4  2021-11-01  I have had countless problems using DoorDash s...       1   \n",
       "\n",
       "        app  label  \n",
       "0  DoorDash      0  \n",
       "1  DoorDash      1  \n",
       "2  DoorDash      1  \n",
       "3  DoorDash      1  \n",
       "4  DoorDash      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae770f7",
   "metadata": {},
   "source": [
    "## 1. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f956a36",
   "metadata": {},
   "source": [
    "Check the dataset size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f9a3f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626377\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e629cae",
   "metadata": {},
   "source": [
    "And the type of apps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d82e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DoorDash', 'UberEats', 'Deliveroo', 'MenuLog', 'Grubhub']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_list = list(reviews['app'].unique())\n",
    "app_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180a133",
   "metadata": {},
   "source": [
    "Let's also get a sense of our dataset's balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fd33ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.662462\n",
       "1    0.337538\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1feda2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.760423\n",
      "1    0.239577\n",
      "Name: label, dtype: float64\n",
      "0    0.648032\n",
      "1    0.351968\n",
      "Name: label, dtype: float64\n",
      "0    0.651189\n",
      "1    0.348811\n",
      "Name: label, dtype: float64\n",
      "0    0.682461\n",
      "1    0.317539\n",
      "Name: label, dtype: float64\n",
      "0    0.56053\n",
      "1    0.43947\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# By app\n",
    "\n",
    "for app in app_list:\n",
    "    print(reviews[reviews['app'] == app]['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a556a3",
   "metadata": {},
   "source": [
    "Across the board the distribution of positive and negative reviews are quite consistent between the apps. Overall, there's an imbalance in our dataset, with positive reviews making for 75% of the dataset. Let's also check for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd9c2c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date       0\n",
       "review    54\n",
       "rating     0\n",
       "app        0\n",
       "label      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb8b89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2701e569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iâ€™ve been a DoorDash user for a while now and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ordered a meal for delivery and after 1:30 I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have gotten three orders from Doordash, all ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The delay and customer support I experienced w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have had countless problems using DoorDash s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  Iâ€™ve been a DoorDash user for a while now and ...      0\n",
       "1  I ordered a meal for delivery and after 1:30 I...      1\n",
       "2  I have gotten three orders from Doordash, all ...      1\n",
       "3  The delay and customer support I experienced w...      1\n",
       "4  I have had countless problems using DoorDash s...      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proc = reviews.copy()\n",
    "df_proc.drop(columns=['date', 'rating', 'app'], inplace=True)\n",
    "df_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea35db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_proc['review']\n",
    "y = df_proc['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a246c2e",
   "metadata": {},
   "source": [
    "## 2. Train Corpus on Word2Vec Model\n",
    "\n",
    "It's unclear to me whether we should create the word2vec model on the entire corpus, or just the train dataset. From 655 class it seems the entire corpus was used, so let's give it a shot.\n",
    "\n",
    "Also referencing: https://github.com/nadbordrozd/blog_stuff/blob/master/classification_w2v/benchmarking_python3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "991c7152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/meln/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20cfe4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce4403be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 626323/626323 [00:05<00:00, 107859.88it/s]\n"
     ]
    }
   ],
   "source": [
    "all_tokenized_reviews = []\n",
    "\n",
    "for review in tqdm(X):\n",
    "    tokens = [token for token in re.findall(r'\\w+', review) if token not in stop_words]\n",
    "    all_tokenized_reviews.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69336a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = Word2Vec(sentences=all_tokenized_reviews, vector_size=100, \n",
    "                      window=2, min_count=100, workers=4, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60c39a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.save(\"word2vec_au.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6213fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_kv = full_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e23ba",
   "metadata": {},
   "source": [
    "We will split the dataset into `train`, `test`, and `dev`, with 80%, 10%, 10% ratio, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "001cc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_test, X_dev, y_test, y_dev = train_test_split(X_test, y_test, test_size=0.5, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "676ca2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501058"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b338cf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62633"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e946dbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62632"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acc8ec94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice apk but some restaurants supply bad quality food'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c23ad78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dope food app'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70a46a",
   "metadata": {},
   "source": [
    "## 3. Word Embeddings Approach on Logistic Regression\n",
    "\n",
    "This section explores the use of word embeddings as feature extraction. We'll be working with dense representations of documents instead of the bag-of-words representations we used earlier. To do this, we'll use the average (or mean) word vector of a document and classify from those representations.\n",
    "\n",
    "As a first step, let's tokenize the reviews here using regular expressions. However, since we're going to be computing an average word vector, let's remove stop words. Here, we'll use NLTK's list of English stop words. Since these words shouldn't affect our classification decision, we can remove them to avoid adding any noisy they might cause. Note that all of the stopwords in NLTK's list are lower-cased, but it's possible that some stopwords in your documents are not entirely lower-cased, so they may not match without some further processing.\n",
    "\n",
    "We'll be using our corpus to train the model. We'll also be using a few of Word2Vec's pre-trained models, `word2vec-google-news-300`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "632a75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnews = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6d721e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnews.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b239ccab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove_small = gensim.downloader.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4c4fe50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_small.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f33fa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================================--] 97.3% 365.8/376.1MB downloaded"
     ]
    }
   ],
   "source": [
    "glove_big = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59205e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_big.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "350fec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dense_features(tokenized_texts, word_vectors): \n",
    "    #HINT: Create an empty list to hold your results \n",
    "        #HINT:Iterate through each item in tokenized_text\n",
    "            #HINT:Create a list that contains current item(s) if found in word_vectors\n",
    "            #HINT:if the length of this list is greater than zero:\n",
    "                #HINT:We set this as a feature, this is done by using numpyâ€™s mean function and append it to our results list \n",
    "            #HINT:Otherwise: create a vector of numpy zeros using word_vectors.vector_size as the parameter and append it to the results list\n",
    "    #HINT:Return the results list as a numpy array (data type)\n",
    "\n",
    "    res = []\n",
    "    for token in tokenized_texts:\n",
    "        items_in_vocab = [item for item in token if item in word_vectors]\n",
    "        if len(items_in_vocab) > 0:\n",
    "            res.append(np.mean(word_vectors[items_in_vocab], axis=0))\n",
    "        else:\n",
    "            res.append(np.zeros(word_vectors.vector_size))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64b5b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501058/501058 [00:04<00:00, 100831.14it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_items = []\n",
    "for review in tqdm(X_train):\n",
    "    tokens = [token for token in re.findall(r'\\w+', review) if token not in stop_words]\n",
    "    tokenized_train_items.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4c73808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokenized_train_items[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db1c797c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nice', 'apk', 'restaurants', 'supply', 'bad', 'quality', 'food']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3ed3d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62633/62633 [00:00<00:00, 136940.28it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dev_items = []\n",
    "for review in tqdm(X_dev):\n",
    "    tokens = [token for token in re.findall(r'\\w+', review) if token not in stop_words]\n",
    "    tokenized_dev_items.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "154941b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(clf):\n",
    "    print(\"_\" * 80)\n",
    "    print(\"Training: \")\n",
    "    clf.fit(X_train_wp, y_train)\n",
    "    y_dev_pred = clf.predict(X_dev_wp)\n",
    "    \n",
    "    score = accuracy_score(y_dev, y_dev_pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "    \n",
    "    print(\"classification report:\")\n",
    "    print(classification_report(y_dev, y_dev_pred))\n",
    "    \n",
    "    print(\"confusion matrix:\")\n",
    "    print(confusion_matrix(y_dev, y_dev_pred))\n",
    "    print(\"Training Complete\")\n",
    "    print()\n",
    "    \n",
    "    clf_descr = str(clf).split(\"(\")[0]\n",
    "    return clf_descr, score, y_dev_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75686f55",
   "metadata": {},
   "source": [
    "It's training time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad5922e",
   "metadata": {},
   "source": [
    "### 3.1 Word2Vec on reviews corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50fbeacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, full_model_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42935e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501058, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_wp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0de54b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_wp = generate_dense_features(tokenized_dev_items, full_model_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3cf8180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62633, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_dev_wp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6228c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training Results\n",
      "Uniform Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.499\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.50      0.57     41413\n",
      "           1       0.34      0.50      0.40     21220\n",
      "\n",
      "    accuracy                           0.50     62633\n",
      "   macro avg       0.50      0.50      0.49     62633\n",
      "weighted avg       0.55      0.50      0.51     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[20678 20735]\n",
      " [10627 10593]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results\n",
      "Most Frequent Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.661\n",
      "classification report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.80     41413\n",
      "           1       0.00      0.00      0.00     21220\n",
      "\n",
      "    accuracy                           0.66     62633\n",
      "   macro avg       0.33      0.50      0.40     62633\n",
      "weighted avg       0.44      0.66      0.53     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[41413     0]\n",
      " [21220     0]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results\n",
      "Logistic Regression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.891\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91     41413\n",
      "           1       0.79      0.93      0.85     21220\n",
      "\n",
      "    accuracy                           0.89     62633\n",
      "   macro avg       0.87      0.90      0.88     62633\n",
      "weighted avg       0.90      0.89      0.89     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[36041  5372]\n",
      " [ 1475 19745]]\n",
      "Training Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = {} # A dict to store our dev set predictions\n",
    "\n",
    "for clf, name in (\n",
    "    (DummyClassifier(strategy='uniform', random_state=RANDOM_SEED), \"Uniform Classifier\"),\n",
    "    (DummyClassifier(strategy='most_frequent', random_state=RANDOM_SEED), \"Most Frequent Classifier\"),\n",
    "    (LogisticRegression(solver='lbfgs', class_weight='balanced', max_iter=1000, random_state=RANDOM_SEED), \"Logistic Regression\")\n",
    "):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Training Results\")\n",
    "    print(name)\n",
    "    mod = train_model(clf)\n",
    "    preds[name] = mod[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db78c7",
   "metadata": {},
   "source": [
    "Pretty good result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e6fae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for mis-classifications\n",
    "def create_mis_classification_df(name):\n",
    "    mis_class = pd.DataFrame(X_dev)\n",
    "    mis_class['Actual'] = y_dev\n",
    "    mis_class['Predicted'] = preds[name]\n",
    "    mis_class = mis_class[mis_class['Actual'] != mis_class['Predicted']]\n",
    "    return mis_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48696732",
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_class_logreg = create_mis_classification_df('Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18c31000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"Orders are generally not as advertised, recently vehicles don't show on map. Seema it has gone down quality.\",\n",
       "        0, 1],\n",
       "       ['Not everywhere in my area yet but getting there and very nice app',\n",
       "        0, 1],\n",
       "       ['I got my chic FIL e', 0, 1],\n",
       "       [\"I no technophobe but even I could navigate through this app and for someone of my era that's saying something!\",\n",
       "        0, 1],\n",
       "       [\"3 star cause why there is not internet banking ??? I don't like to add cards in apps ... Internet banking is more handy for me but it's not available..\",\n",
       "        0, 1],\n",
       "       ['Lately the app has been playing up, pictures and stores wont show up. Also feel like the prices of delivery have gone up which is why i havnt used the app as much lately.',\n",
       "        0, 1],\n",
       "       [\"The app runs perfectly and I've never had a problem with the actual app, my only problem is the massive price increase on the food compared to at the restaurant, it's almost double the price excluding delivery and service fee.\",\n",
       "        0, 1],\n",
       "       [\"Can't wait to see if we like it!\", 0, 1],\n",
       "       ['Does what it says on the box couldnt be happier.', 0, 1],\n",
       "       [\"Great app. Customer service kind of sucks though it's nice and quick but they only sometimes refund you when they screw up. Most of the time they just credit you like 10 bucks even though that does not pay for the time spent waiting for food.\",\n",
       "        0, 1]], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis_class_logreg.sample(10).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8f1fc",
   "metadata": {},
   "source": [
    "### 3.2 Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a59dae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, gnews)\n",
    "X_dev_wp = generate_dense_features(tokenized_dev_items, gnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9162ee10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training Results\n",
      "Logistic Regression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.879\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90     41413\n",
      "           1       0.77      0.93      0.84     21220\n",
      "\n",
      "    accuracy                           0.88     62633\n",
      "   macro avg       0.86      0.89      0.87     62633\n",
      "weighted avg       0.89      0.88      0.88     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[35404  6009]\n",
      " [ 1570 19650]]\n",
      "Training Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = {} # A dict to store our dev set predictions\n",
    "print(\"=\" * 80)\n",
    "print(\"Training Results\")\n",
    "print(\"Logistic Regression\")\n",
    "clf = LogisticRegression(solver='lbfgs', class_weight='balanced', max_iter=1000, random_state=RANDOM_SEED)\n",
    "mod = train_model(clf)\n",
    "preds[name] = mod[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa9e21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_class_logreg_gnews = create_mis_classification_df('Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "682bfe1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Very difficult to use. Frustrating and limited choice.', 1, 0],\n",
       "       [\"I changed my phone and number..I logged onto my old phone, changed my details.. And when at checkout.. It won't let me ðŸ¤£\",\n",
       "        0, 1],\n",
       "       [\"Nicely delivered, but overpriced. I wish Uber eats would stop charging bs fees. A delivery fee and a service fee, forgot to mention on top of that you gotta pay a tip. Seriously ! I'm fine with tips, but everything else is bs.\",\n",
       "        0, 1],\n",
       "       ['Had trouble adding payment method', 0, 1],\n",
       "       [\"It charged me twice for one order! I had to jump through hoops to get a refund and I was refunded everything but the tip! Why should I be charged a tip fee for something that never arrived? I won't be ordering through uber eats again\",\n",
       "        0, 1],\n",
       "       ['I do like this app, but for some reason, in some cases, the app menu does not offer all of the items (or the customizations) that you can order directly from the restaurant website.',\n",
       "        0, 1],\n",
       "       ['too expensive', 0, 1],\n",
       "       ['Better than postmates, grub trash and door trash', 0, 1],\n",
       "       ['Very baad', 1, 0],\n",
       "       [\"My order never arrived and I still haven't received a refund for the missing order, this service doesn't care about its customers\",\n",
       "        0, 1]], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis_class_logreg_gnews.sample(10).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411de11c",
   "metadata": {},
   "source": [
    "### 3.3 Glove Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d19e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, glove_small)\n",
    "X_dev_wp = generate_dense_features(tokenized_dev_items, glove_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5402a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training Results\n",
      "Logistic Regression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.826\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.81      0.86     41413\n",
      "           1       0.70      0.86      0.77     21220\n",
      "\n",
      "    accuracy                           0.83     62633\n",
      "   macro avg       0.81      0.84      0.82     62633\n",
      "weighted avg       0.84      0.83      0.83     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[33424  7989]\n",
      " [ 2893 18327]]\n",
      "Training Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = {} # A dict to store our dev set predictions\n",
    "print(\"=\" * 80)\n",
    "print(\"Training Results\")\n",
    "print(\"Logistic Regression\")\n",
    "clf = LogisticRegression(solver='lbfgs', class_weight='balanced', max_iter=1000, random_state=RANDOM_SEED)\n",
    "mod = train_model(clf)\n",
    "preds[name] = mod[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3274c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_class_logreg_glove_small = create_mis_classification_df('Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2cae2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Excelente opcion para los que no tienen tiempo de salir a comer. Ordenar es extremadamente sencillo.. Ya no mas orden incorrecta porque la persona en el telefono no entendiÃ³ bien!!',\n",
       "        0, 1],\n",
       "       [\"It is so hard to use, my home address/delivery details get deleted with each new update. I have seen my rating drop because the delivery person cannot access my apartment as they need a buzzer code which each update removes. The GPS doesn't always work so tracking orders is hard. I didn't realize food was delivered until I received the noticed that it had as I couldn't track it.\",\n",
       "        0, 1],\n",
       "       ['Works without fail. Brilliant for lazy buggers like me or just those that are time deprived. I order on the way home from work to a usual short wait.',\n",
       "        0, 1],\n",
       "       [\"If you want an app that'll pester you, sometimes multiple times a day, with useless adds for their own services, then you're in the right place. Many of the promotions they are 'offering' are only good at one retailer and they don't tell you which one, it's up to you to scroll. There are other cheaper and simpler apps.\",\n",
       "        1, 0],\n",
       "       ['En esta pandemia de Covid-19 ha sido muy util ya que todo esta cerrado en mi lugar de trabajo Graias UBER EAT',\n",
       "        0, 1],\n",
       "       [\"I don't have a license and I don't have food but I do have money\",\n",
       "        0, 1],\n",
       "       ['Nice aggregator app, a lot of restaurants in the base, fast processing',\n",
       "        0, 1],\n",
       "       ['Ubereats is the best they be right on time and if the order is wrong you get money back and you get like discounts on your order',\n",
       "        0, 1],\n",
       "       ['It has been acquired by Zomato as of today in India and no longer functional. I miss this interface though',\n",
       "        0, 1],\n",
       "       [\"Used it for the first time tonight and it wouldn't give me my 25% discount so I had to use my daughter's phone\",\n",
       "        0, 1]], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis_class_logreg_glove_small.sample(10).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f85773",
   "metadata": {},
   "source": [
    "### 3.4 Glove Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de0f0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, glove_big)\n",
    "X_dev_wp = generate_dense_features(tokenized_dev_items, glove_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e78bd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training Results\n",
      "Logistic Regression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.849\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88     41413\n",
      "           1       0.73      0.88      0.80     21220\n",
      "\n",
      "    accuracy                           0.85     62633\n",
      "   macro avg       0.83      0.86      0.84     62633\n",
      "weighted avg       0.86      0.85      0.85     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[34405  7008]\n",
      " [ 2477 18743]]\n",
      "Training Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = {} # A dict to store our dev set predictions\n",
    "print(\"=\" * 80)\n",
    "print(\"Training Results\")\n",
    "print(\"Logistic Regression\")\n",
    "clf = LogisticRegression(solver='lbfgs', class_weight='balanced', max_iter=1000, random_state=RANDOM_SEED)\n",
    "mod = train_model(clf)\n",
    "preds[name] = mod[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "55cec0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_class_logreg_glove_big = create_mis_classification_df('Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "99fbb4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"I ordered bread today from Cobb's bakery & was basically sent stale stuff! When I 1st complained to uber eats they said they couldn't give me a refund because I didn't provide enough information! I don't know what else I was supposed to do! I tried to reach out again & I've been told their team has to review my complaint! That was over 2 hours ago! How long does it take to get a refund! Got a feeling I've getting screwed!\",\n",
       "        0, 1],\n",
       "       ['It has been a lifesaver this year. And the couple times there was a error, a missing item. They refunded me right away. Love it when they offer specials!',\n",
       "        0, 1],\n",
       "       ['need to improve on delivery time', 1, 0],\n",
       "       [\"Couldn't any more info.. UP?\", 1, 0],\n",
       "       [\"I'm getting fat. Thanks Uber Eats! :)\", 0, 1],\n",
       "       ['Certain items are not included in some store/restaurants menus. I donâ€™t know if this is an issue caused by Menulog or the individual stores, but I find it annoying either way.',\n",
       "        0, 1],\n",
       "       ['there is no way to report missing items on the orders', 0, 1],\n",
       "       ['vera level', 0, 1],\n",
       "       ['Wish there was a feature to search by hygiene rating', 0, 1],\n",
       "       ['Never disappointed', 0, 1]], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis_class_logreg_glove_big.sample(10).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82cfe54",
   "metadata": {},
   "source": [
    "### 3.5 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe5db10",
   "metadata": {},
   "source": [
    "Perhaps unsurprisingly, the model achieved the highest macro average F1-score on the Word2Vec model trained using the reviews corpus. However, it's also striking to see that pre-trained Word2Vec models did just as well. Gnews for example, achieved a F1-score of 0.87, while Glove Big achieved 0.84. While this is still inferior to the bag-of-words approach, it shows just how effective the word embeddings method can be.\n",
    "\n",
    "Unfortunately, it is a lot harder to understand the mis-classifications. Except for those reviews where the rating given isn't an accurate reflection of the sentiment, it's hard to understand the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8d9835",
   "metadata": {},
   "source": [
    "### 3.6 Tune Logistic Regression Params\n",
    "\n",
    "We'll try tuning Logistic Regression C parameter again, similar to BOW. I really like the fact that Google News embeddings did so well, so let's use that as the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2fa2f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, gnews)\n",
    "X_dev_wp = generate_dense_features(tokenized_dev_items, gnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbdc5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LogReg Grid Search\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='lbfgs', class_weight='balanced', max_iter=3000, random_state=RANDOM_SEED)\n",
    "param_grid = {'C' : np.logspace(-4, 4, 20)}\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LogReg Grid Search\")\n",
    "clf = GridSearchCV(clf, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "best_clf = clf.fit(X_train_wp, y_train)\n",
    "print(clf.best_params_)\n",
    "y_dev_pred = clf.predict(X_dev_wp)\n",
    "score = accuracy_score(y_dev, y_dev_pred)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "print(\"classification report:\")\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_dev, y_dev_pred))\n",
    "print(\"Training Complete\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f02387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
