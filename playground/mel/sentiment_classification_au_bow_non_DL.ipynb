{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21330d8",
   "metadata": {},
   "source": [
    "## Sentiment Classification AU Reviews Data (BOW, non-Deep Learning)\n",
    "\n",
    "This notebook covers two good approaches to perform sentiment classification - Naive Bayes and Logistic Regression. We will train AU reviews data on both.\n",
    "\n",
    "As a rule of thumb, reviews that are 3 stars and above are **positive**, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd1171c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/meln/.local/lib/python3.8/site-packages (3.2.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/meln/.local/lib/python3.8/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/meln/.local/lib/python3.8/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/meln/.local/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/meln/.local/lib/python3.8/site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/meln/.local/lib/python3.8/site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: jinja2 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/meln/.local/lib/python3.8/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/meln/.local/lib/python3.8/site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/meln/.local/lib/python3.8/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/meln/.local/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/meln/.local/lib/python3.8/site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/meln/.local/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/meln/.local/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/meln/.local/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/meln/.local/lib/python3.8/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/meln/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a09b873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /home/meln/.local/lib/python3.8/site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/meln/.local/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from gensim) (1.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b213884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy_langdetect in /home/meln/.local/lib/python3.8/site-packages (0.1.2)\n",
      "Requirement already satisfied: pytest in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from spacy_langdetect) (6.2.3)\n",
      "Requirement already satisfied: langdetect==1.0.7 in /home/meln/.local/lib/python3.8/site-packages (from spacy_langdetect) (1.0.7)\n",
      "Requirement already satisfied: six in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from langdetect==1.0.7->spacy_langdetect) (1.15.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from pytest->spacy_langdetect) (20.3.0)\n",
      "Requirement already satisfied: iniconfig in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from pytest->spacy_langdetect) (1.1.1)\n",
      "Requirement already satisfied: packaging in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from pytest->spacy_langdetect) (20.9)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from pytest->spacy_langdetect) (0.13.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from pytest->spacy_langdetect) (1.10.0)\n",
      "Requirement already satisfied: toml in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from pytest->spacy_langdetect) (0.10.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from packaging->pytest->spacy_langdetect) (2.4.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy_langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad89d88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 16.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /home/meln/.local/lib/python3.8/site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (20.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/meln/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/meln/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/meln/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/meln/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/meln/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/meln/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/meln/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/meln/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/meln/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: setuptools in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/meln/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/meln/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/meln/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.1)\n",
      "Requirement already satisfied: jinja2 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/meln/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.59.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/meln/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/meln/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2020.12.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "926204c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import spacy\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.language import Language\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22215586",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1fcb6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>app</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>I’ve been a DoorDash user for a while now and ...</td>\n",
       "      <td>3</td>\n",
       "      <td>DoorDash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>I ordered a meal for delivery and after 1:30 I...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-03</td>\n",
       "      <td>I have gotten three orders from Doordash, all ...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>The delay and customer support I experienced w...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>I have had countless problems using DoorDash s...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                             review  rating  \\\n",
       "0  2020-07-11  I’ve been a DoorDash user for a while now and ...       3   \n",
       "1  2020-05-26  I ordered a meal for delivery and after 1:30 I...       1   \n",
       "2  2020-09-03  I have gotten three orders from Doordash, all ...       1   \n",
       "3  2021-08-13  The delay and customer support I experienced w...       1   \n",
       "4  2021-11-01  I have had countless problems using DoorDash s...       1   \n",
       "\n",
       "        app  \n",
       "0  DoorDash  \n",
       "1  DoorDash  \n",
       "2  DoorDash  \n",
       "3  DoorDash  \n",
       "4  DoorDash  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_pickle(\"assets/au_reviews.pkl\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49be95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['label'] = np.where(reviews['rating'] >= 3, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "570595bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>app</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>I’ve been a DoorDash user for a while now and ...</td>\n",
       "      <td>3</td>\n",
       "      <td>DoorDash</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>I ordered a meal for delivery and after 1:30 I...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-03</td>\n",
       "      <td>I have gotten three orders from Doordash, all ...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>The delay and customer support I experienced w...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>I have had countless problems using DoorDash s...</td>\n",
       "      <td>1</td>\n",
       "      <td>DoorDash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                             review  rating  \\\n",
       "0  2020-07-11  I’ve been a DoorDash user for a while now and ...       3   \n",
       "1  2020-05-26  I ordered a meal for delivery and after 1:30 I...       1   \n",
       "2  2020-09-03  I have gotten three orders from Doordash, all ...       1   \n",
       "3  2021-08-13  The delay and customer support I experienced w...       1   \n",
       "4  2021-11-01  I have had countless problems using DoorDash s...       1   \n",
       "\n",
       "        app  label  \n",
       "0  DoorDash      0  \n",
       "1  DoorDash      1  \n",
       "2  DoorDash      1  \n",
       "3  DoorDash      1  \n",
       "4  DoorDash      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae770f7",
   "metadata": {},
   "source": [
    "## 1. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f956a36",
   "metadata": {},
   "source": [
    "Check the dataset size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f9a3f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626377\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e629cae",
   "metadata": {},
   "source": [
    "And the type of apps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64d82e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DoorDash', 'UberEats', 'Deliveroo', 'MenuLog', 'Grubhub']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_list = list(reviews['app'].unique())\n",
    "app_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180a133",
   "metadata": {},
   "source": [
    "Let's also get a sense of our dataset's balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fd33ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.662462\n",
       "1    0.337538\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1feda2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.760423\n",
      "1    0.239577\n",
      "Name: label, dtype: float64\n",
      "0    0.648032\n",
      "1    0.351968\n",
      "Name: label, dtype: float64\n",
      "0    0.651189\n",
      "1    0.348811\n",
      "Name: label, dtype: float64\n",
      "0    0.682461\n",
      "1    0.317539\n",
      "Name: label, dtype: float64\n",
      "0    0.56053\n",
      "1    0.43947\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# By app\n",
    "\n",
    "for app in app_list:\n",
    "    print(reviews[reviews['app'] == app]['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a556a3",
   "metadata": {},
   "source": [
    "Across the board the distribution of positive and negative reviews are quite consistent between the apps. Overall, there's an imbalance in our dataset, with positive reviews making for 75% of the dataset. Let's also check for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd9c2c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date       0\n",
       "review    54\n",
       "rating     0\n",
       "app        0\n",
       "label      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb8b89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2701e569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’ve been a DoorDash user for a while now and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ordered a meal for delivery and after 1:30 I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have gotten three orders from Doordash, all ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The delay and customer support I experienced w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have had countless problems using DoorDash s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  I’ve been a DoorDash user for a while now and ...      0\n",
       "1  I ordered a meal for delivery and after 1:30 I...      1\n",
       "2  I have gotten three orders from Doordash, all ...      1\n",
       "3  The delay and customer support I experienced w...      1\n",
       "4  I have had countless problems using DoorDash s...      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proc = reviews.copy()\n",
    "df_proc.drop(columns=['date', 'rating', 'app'], inplace=True)\n",
    "df_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0afa2e",
   "metadata": {},
   "source": [
    "For AU dataset we won't be filtering out non-English reviews. It's likely that this makes up for a very small proportion of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df12e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc.to_csv('reviews_au_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea35db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_proc['review']\n",
    "y = df_proc['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e23ba",
   "metadata": {},
   "source": [
    "We will split the dataset into `train`, `test`, and `dev`, with 80%, 10%, 10% ratio, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "001cc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_test, X_dev, y_test, y_dev = train_test_split(X_test, y_test, test_size=0.5, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "676ca2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501058"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b338cf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62633"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e946dbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62632"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acc8ec94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice apk but some restaurants supply bad quality food'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c23ad78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dope food app'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70a46a",
   "metadata": {},
   "source": [
    "## 2.1 Bag-of-Words Approach on Naive Bayes & Logistic Regression\n",
    "\n",
    "This section explores the use of bag of words as feature extraction. But first, let's have a look at the token frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2550abec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501058/501058 [00:20<00:00, 24120.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fill this with any token (with anything in it!) for tokens separated by whitespace\n",
    "ws_tokens = Counter()\n",
    "\n",
    "# Fill this one with tokens separated by whitespace but constisting only of tokens\n",
    "# that are totally made of alphanumeric characters (you can use the \\w character\n",
    "# class in making the regex)\n",
    "\n",
    "alpha_ws_tokens = Counter()\n",
    "\n",
    "# Fill this one with the tokens separated by *word boundaries* (not white space) that consist\n",
    "# of alphanumeric characters (use \\w again)\n",
    "alpha_re_tokens = Counter()\n",
    "for review in tqdm(X_train):\n",
    "    ws_review = review.split()\n",
    "    ws_tokens.update(ws_review)\n",
    "    # Note: use fullmatch() as it anchor both the start and end of str. match() won't work.\n",
    "    alpha_ws_tokens.update([re.fullmatch(r'\\w+', word).group() for word in ws_review if re.fullmatch(r'\\w+', word) != None])\n",
    "    alpha_re_tokens.update(re.findall(r'\\w+', review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5196b087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221265\n",
      "89320\n",
      "103276\n"
     ]
    }
   ],
   "source": [
    "print(len(ws_tokens))\n",
    "print(len(alpha_ws_tokens))\n",
    "print(len(alpha_re_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84d1db31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 266849),\n",
       " ('to', 255887),\n",
       " ('and', 227093),\n",
       " ('I', 209984),\n",
       " ('a', 134290),\n",
       " ('it', 132177),\n",
       " ('app', 128152),\n",
       " ('is', 113100),\n",
       " ('order', 105566),\n",
       " ('my', 103000),\n",
       " ('for', 92448),\n",
       " ('food', 86756),\n",
       " ('t', 86739),\n",
       " ('of', 77926),\n",
       " ('you', 73536),\n",
       " ('service', 71883),\n",
       " ('not', 66109),\n",
       " ('delivery', 65577),\n",
       " ('they', 62087),\n",
       " ('in', 60542),\n",
       " ('have', 58140),\n",
       " ('but', 56996),\n",
       " ('was', 56483),\n",
       " ('that', 55889),\n",
       " ('this', 55238),\n",
       " ('with', 55091),\n",
       " ('me', 52427),\n",
       " ('on', 52145),\n",
       " ('good', 50797),\n",
       " ('time', 49030),\n",
       " ('use', 46261),\n",
       " ('s', 46192),\n",
       " ('no', 40860),\n",
       " ('are', 40347),\n",
       " ('get', 38224),\n",
       " ('from', 37020),\n",
       " ('your', 35031),\n",
       " ('an', 34730),\n",
       " ('so', 34695),\n",
       " ('i', 34579),\n",
       " ('when', 34508),\n",
       " ('be', 33970),\n",
       " ('can', 32035),\n",
       " ('or', 30616),\n",
       " ('Uber', 29980),\n",
       " ('customer', 29608),\n",
       " ('Great', 29182),\n",
       " ('up', 29072),\n",
       " ('The', 27958),\n",
       " ('very', 27315),\n",
       " ('had', 26841),\n",
       " ('Good', 26628),\n",
       " ('at', 26498),\n",
       " ('as', 25374),\n",
       " ('just', 25343),\n",
       " ('great', 24541),\n",
       " ('if', 24290),\n",
       " ('It', 23879),\n",
       " ('all', 23295),\n",
       " ('uber', 22767),\n",
       " ('even', 22473),\n",
       " ('eats', 22425),\n",
       " ('there', 21815),\n",
       " ('easy', 21722),\n",
       " ('never', 21544),\n",
       " ('don', 20550),\n",
       " ('will', 20467),\n",
       " ('Very', 20196),\n",
       " ('restaurant', 20038),\n",
       " ('driver', 19911),\n",
       " ('more', 19794),\n",
       " ('like', 19277),\n",
       " ('money', 18437),\n",
       " ('been', 18336),\n",
       " ('than', 18051),\n",
       " ('then', 18012),\n",
       " ('has', 17747),\n",
       " ('out', 17464),\n",
       " ('after', 17389),\n",
       " ('drivers', 17182),\n",
       " ('orders', 17051),\n",
       " ('always', 17005),\n",
       " ('do', 16717),\n",
       " ('only', 16557),\n",
       " ('refund', 16318),\n",
       " ('times', 15973),\n",
       " ('them', 15685),\n",
       " ('one', 15540),\n",
       " ('delivered', 15460),\n",
       " ('now', 15293),\n",
       " ('They', 15028),\n",
       " ('because', 14873),\n",
       " ('any', 14662),\n",
       " ('restaurants', 14635),\n",
       " ('experience', 14612),\n",
       " ('would', 14560),\n",
       " ('m', 14537),\n",
       " ('again', 14471),\n",
       " ('ordered', 14363),\n",
       " ('love', 14072)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100 = alpha_re_tokens.most_common(100)\n",
    "top_100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc95113",
   "metadata": {},
   "source": [
    "Lots of stopwords, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e6f87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(100))\n",
    "y = [word_tup[1] for word_tup in top_100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "488d0ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.plot(x, y, '.')\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab13949",
   "metadata": {},
   "source": [
    "And unexpectedly, the word frequency distribution also follows Zipf's law as well. What that means is that we can essentially remove uncommon words, without worrying that they will affect performance. We will also need to remove stopwords, and add unigrams and bigrams as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27098b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=500, ngram_range=(1,2))\n",
    "X_train_bow = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42935e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501058, 1389)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0de54b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_bow = vectorizer.transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "154941b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(clf):\n",
    "    print(\"_\" * 80)\n",
    "    print(\"Training: \")\n",
    "    clf.fit(X_train_bow, y_train)\n",
    "    y_dev_pred = clf.predict(X_dev_bow)\n",
    "    \n",
    "    score = accuracy_score(y_dev, y_dev_pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "    \n",
    "    print(\"classification report:\")\n",
    "    print(classification_report(y_dev, y_dev_pred))\n",
    "    \n",
    "    print(\"confusion matrix:\")\n",
    "    print(confusion_matrix(y_dev, y_dev_pred))\n",
    "    print(\"Training Complete\")\n",
    "    print()\n",
    "    \n",
    "    clf_descr = str(clf).split(\"(\")[0]\n",
    "    return clf_descr, score, y_dev_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75686f55",
   "metadata": {},
   "source": [
    "It's training time! We'll start with a few dummy classifiers, followed by Naive Bayes and Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4506c1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training Results - Dummy Classifiers\n",
      "Uniform Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.499\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.50      0.57     41413\n",
      "           1       0.34      0.50      0.40     21220\n",
      "\n",
      "    accuracy                           0.50     62633\n",
      "   macro avg       0.50      0.50      0.49     62633\n",
      "weighted avg       0.55      0.50      0.51     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[20678 20735]\n",
      " [10627 10593]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Dummy Classifiers\n",
      "Most Frequent Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.661\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.80     41413\n",
      "           1       0.00      0.00      0.00     21220\n",
      "\n",
      "    accuracy                           0.66     62633\n",
      "   macro avg       0.33      0.50      0.40     62633\n",
      "weighted avg       0.44      0.66      0.53     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[41413     0]\n",
      " [21220     0]]\n",
      "Training Complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/sw/arcts/centos7/python3.8-anaconda/2021.05/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for clf, name in (\n",
    "    (DummyClassifier(strategy='uniform', random_state=RANDOM_SEED), \"Uniform Classifier\"),\n",
    "    (DummyClassifier(strategy='most_frequent', random_state=RANDOM_SEED), \"Most Frequent Classifier\"),\n",
    "):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Training Results - Dummy Classifiers\")\n",
    "    print(name)\n",
    "    mod = train_model(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6228c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "MultinomialNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.899\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92     41413\n",
      "           1       0.84      0.87      0.85     21220\n",
      "\n",
      "    accuracy                           0.90     62633\n",
      "   macro avg       0.88      0.89      0.89     62633\n",
      "weighted avg       0.90      0.90      0.90     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[37812  3601]\n",
      " [ 2724 18496]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "BernoulliNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.853\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89     41413\n",
      "           1       0.82      0.72      0.77     21220\n",
      "\n",
      "    accuracy                           0.85     62633\n",
      "   macro avg       0.84      0.82      0.83     62633\n",
      "weighted avg       0.85      0.85      0.85     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[38185  3228]\n",
      " [ 6005 15215]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "ComplementNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.885\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91     41413\n",
      "           1       0.78      0.92      0.85     21220\n",
      "\n",
      "    accuracy                           0.89     62633\n",
      "   macro avg       0.87      0.89      0.88     62633\n",
      "weighted avg       0.90      0.89      0.89     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[35873  5540]\n",
      " [ 1641 19579]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "Logistic Regression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.902\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92     41413\n",
      "           1       0.82      0.92      0.86     21220\n",
      "\n",
      "    accuracy                           0.90     62633\n",
      "   macro avg       0.89      0.91      0.89     62633\n",
      "weighted avg       0.91      0.90      0.90     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[37061  4352]\n",
      " [ 1765 19455]]\n",
      "Training Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = {} # A dict to store our dev set predictions\n",
    "\n",
    "for clf, name in (\n",
    "    (MultinomialNB(alpha=0.01), \"MultinomialNB\"),\n",
    "    (BernoulliNB(alpha=0.01), \"BernoulliNB\"),\n",
    "    (ComplementNB(alpha=0.1), \"ComplementNB\"),\n",
    "    (LogisticRegression(solver='lbfgs', class_weight='balanced', max_iter=1000, random_state=RANDOM_SEED), \"Logistic Regression\")\n",
    "):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Training Results - Naive Bayes & LogReg\")\n",
    "    print(name)\n",
    "    mod = train_model(clf)\n",
    "    preds[name] = mod[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7bd9b5",
   "metadata": {},
   "source": [
    "The baseline results are quite promising, with both MultinomialNB and Logistic Regression achieving 0.85 on macro F1-score. This means that the bag-of-word approach is a rather solid approach for sentiment classification. It's also interesting to see that while MultinomialNB has a rather balanced number of false pos and false neg, BernoulliNB and ComplementNB are different. BernoulliNB has a much higher number of fps, while ComplementNB has a much higher number of fns.\n",
    "\n",
    "Also, according to https://web.stanford.edu/~jurafsky/slp3/4.pdf, using binary NB (BernoulliNB) may improve predictive performance, as whether a word occurs or not seems to matter more than its frequency. But in this case, BernoulliNB does not outperform other Naive Bayes methods. We'll come back to this in a second.\n",
    "\n",
    "Let's take a look at a few of mis-classifications for both Naive Bayes and Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e6fae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for mis-classifications\n",
    "def create_mis_classification_df(name):\n",
    "    mis_class = pd.DataFrame(X_dev)\n",
    "    mis_class['Actual'] = y_dev\n",
    "    mis_class['Predicted'] = preds[name]\n",
    "    mis_class = mis_class[mis_class['Actual'] != mis_class['Predicted']]\n",
    "    return mis_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48696732",
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_class_multi = create_mis_classification_df('MultinomialNB')\n",
    "mis_class_bernoulli = create_mis_classification_df('BernoulliNB')\n",
    "mis_class_complement = create_mis_classification_df('ComplementNB')\n",
    "mis_class_logreg = create_mis_classification_df('Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83f232f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"orders don't get double checked and are frequently missing items or just not correct\",\n",
       "        0, 1],\n",
       "       ['Because I said so', 0, 1],\n",
       "       ['Ratas', 1, 0],\n",
       "       ['I always use this app, but recently we found out Uber is overcharging restaurants, taking opportunity of the Pendemic. I shall not use this services again until they justify there actions.',\n",
       "        1, 0],\n",
       "       ['Dont stay stuck while hungry. This app will keep you fed!', 0,\n",
       "        1],\n",
       "       ['Bakwas service', 1, 0],\n",
       "       ['You can trust UberEATS', 0, 1],\n",
       "       [\"Only a stupid person will get trapped with Uber Eats . A pass for food to be delivered that will cost you a Arm and Leg ! Thank you But No Thank you Keep the food I will get my own food and it's not worth it . I love KAM KAU CHINESE FOOD they are not even on the list ... Thanks Uber but you won't hit my pocket 👎\",\n",
       "        1, 0],\n",
       "       [\"The actual reason why I'm fat\", 0, 1],\n",
       "       [\"For the most part. Good service. Sometimes restaurants don't put all items available or don't let you know what's sold out and your order is cancelled. But only happened a couple times.\",\n",
       "        0, 1]], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis_class_multi.sample(10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b1f630c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"Love doordash, but this one doordasher was bad. Didn't get all of what I had order, said he was going get what I was missing, didn't come back.So I still use doordash though.\",\n",
       "        0, 1],\n",
       "       ['The service was great at first during COVID. Now, I constantly get lost or late orders. They point the finger to the store and the store points the finger to DoorDash.',\n",
       "        1, 0],\n",
       "       [\"Overall a convenient app + service, but delivery time can be hit or miss. A lot of drivers have trouble finding the right house/door despite obvious signs and clearly typed instructions for every house I've ordered from. The driver GPS lags pretty bad too. Other than that, most drivers are very polite (and apologetic when they can't find me lol).\",\n",
       "        0, 1],\n",
       "       ['This is mt 2nd time ordering first order was 20 min past est delievry. So i am giving them another try',\n",
       "        0, 1],\n",
       "       [\"I think I got same order I was expecting fried white egg rice & others. Think you have me same re'order I was not happy. Have ordered several times from you. I wish o could get what I actually ordered. Why am I on Google & not Tiscali. J. Oku Quashie.\",\n",
       "        0, 1],\n",
       "       ['ridiculous prices. extreme sexism throughout the entire company',\n",
       "        1, 0],\n",
       "       ['They have good prices compared to some of the other delivery services and I just think their selection is better than any other at the moment. I know every one is improving by the day so this is just the one that I prefer right now, but that may change one day.',\n",
       "        0, 1],\n",
       "       [\"I have been prevented from getting food each time I am in a new country. It's always one issue or another\",\n",
       "        1, 0],\n",
       "       ['Tool forever to find a driver', 1, 0],\n",
       "       ['constant crashes', 1, 0]], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis_class_bernoulli.sample(10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0bb7438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['abe pehele app thik karo phir playstore par dalo', 1, 0],\n",
       "       ['Expensive deals than usual and no discounts and off', 1, 0],\n",
       "       ['1. month', 0, 1],\n",
       "       ['Good but i am unable to change my delivery address even if I add a new one the old is always the default',\n",
       "        0, 1],\n",
       "       ['Not good ,', 1, 0],\n",
       "       [\"It's alright but when my order got cancelled I didn't get any notification which was pretty annoying. Editing an address is also a bit clunky. Moreover, in some of the menus it's not obvious if an item is an extra or not: like with a local burger place you can add toppings and beef patty but it doesn't say if those are extras so if I don't add them does it mean my burger would come without any patty or toppings? Just for comparison, JustEat makes it clear in the same restaurant's menu.\",\n",
       "        0, 1],\n",
       "       [\"I could tip the drivers a lot better if the applications fees weren't so high\",\n",
       "        1, 0],\n",
       "       ['Good, easy to use app. However, the customer support response time is very slow when you have reported a problem with your order.',\n",
       "        1, 0],\n",
       "       ['Ubereats is the best they be right on time and if the order is wrong you get money back and you get like discounts on your order',\n",
       "        0, 1],\n",
       "       ['Show more pictures for Chinese descriptions or add English to Chinese labels.',\n",
       "        0, 1]], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis_class_complement.sample(10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18c31000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Sometimes an hour late with cold food', 0, 1],\n",
       "       ['this app keeps bugging me to rate it everytime i use it, so im doing this hoping to never get notifications again. Small issue is that if i buy an uber eats subscription, and it says \"Spend 80 to get free delivery\" restaurants with somethin close to that price will make their prices like R79.90',\n",
       "        0, 1],\n",
       "       ['Create', 0, 1],\n",
       "       ['Not bad noice size', 0, 1],\n",
       "       ['Come back uber eats in India we are missing you a lot', 0, 1],\n",
       "       [\"All's good but I cannot fathom how they manage to operate legally in Sri Lanka without having a Hotline or any customer support system to get immediate help from!\",\n",
       "        0, 1],\n",
       "       [\"Can always count on uber to get me food that don't offer delivery.\",\n",
       "        0, 1],\n",
       "       [\"Just ordered for the first time and were very excited. Our driver had difficulties dropping off the food and called us, perfect. However our driver then drive off with our food and marked the deliver as complete. Company responded with refunded credit, though disappointed I've now waited 2 hours for dinner I am satisfied that the company responded and we were not charged for a meal that never came.\",\n",
       "        0, 1],\n",
       "       ['Está bien, te dan lo que ofrecen y nada más allá.', 0, 1],\n",
       "       ['Issue was fixed thank you', 1, 0]], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis_class_logreg.sample(10).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3993d",
   "metadata": {},
   "source": [
    "Really interesting. Looking at the results, there are a few cases of mis-classifications:\n",
    "- Reviews that contain negation expressions, eg \"not okay\" is classified as a positive review when in reality it should be negative. BoW approach makes it hard for ML model to recognize this kind of expressions.\n",
    "- Reviews that are mis-classified due to rating. Eg a customer may write something negative but give 3 stars. It's tricky in this case because it's a caveat of our dataset.\n",
    "- Some mis-classification is the ML model being weirdly off, eg ComplementNB classified a \"Good\" review as negative, or reviews containing the word 'hate' gets classified as positive.\n",
    "- Contextual awareness is important and this is something that bag-of-word approaches cannot address. For example the sentence 'Very good. Expensive delivery charge though' gets classified as negative likely because of the word expensive, while in reality this is a positive review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02b0da",
   "metadata": {},
   "source": [
    "### 2.1 Reduce min_df\n",
    "\n",
    "We set a min frequency cap of 500. What happens if we reduce this cap to 100?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3069fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=100, ngram_range=(1,2))\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_dev_bow = vectorizer.transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04437490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "MultinomialNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.907\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93     41413\n",
      "           1       0.84      0.89      0.87     21220\n",
      "\n",
      "    accuracy                           0.91     62633\n",
      "   macro avg       0.89      0.90      0.90     62633\n",
      "weighted avg       0.91      0.91      0.91     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[37866  3547]\n",
      " [ 2276 18944]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "BernoulliNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.856\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89     41413\n",
      "           1       0.83      0.72      0.77     21220\n",
      "\n",
      "    accuracy                           0.86     62633\n",
      "   macro avg       0.85      0.82      0.83     62633\n",
      "weighted avg       0.85      0.86      0.85     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[38298  3115]\n",
      " [ 5888 15332]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "ComplementNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.896\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.92     41413\n",
      "           1       0.79      0.94      0.86     21220\n",
      "\n",
      "    accuracy                           0.90     62633\n",
      "   macro avg       0.88      0.91      0.89     62633\n",
      "weighted avg       0.91      0.90      0.90     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[36232  5181]\n",
      " [ 1343 19877]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "Logistic Regression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.913\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93     41413\n",
      "           1       0.83      0.93      0.88     21220\n",
      "\n",
      "    accuracy                           0.91     62633\n",
      "   macro avg       0.90      0.92      0.90     62633\n",
      "weighted avg       0.92      0.91      0.91     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[37458  3955]\n",
      " [ 1517 19703]]\n",
      "Training Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = {} # A dict to store our dev set predictions\n",
    "\n",
    "for clf, name in (\n",
    "    (MultinomialNB(alpha=0.01), \"MultinomialNB\"),\n",
    "    (BernoulliNB(alpha=0.01), \"BernoulliNB\"),\n",
    "    (ComplementNB(alpha=0.1), \"ComplementNB\"),\n",
    "    (LogisticRegression(solver='lbfgs', class_weight='balanced', max_iter=1000, random_state=RANDOM_SEED), \"Logistic Regression\")\n",
    "):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Training Results - Naive Bayes & LogReg\")\n",
    "    print(name)\n",
    "    mod = train_model(clf)\n",
    "    preds[name] = mod[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43752fc7",
   "metadata": {},
   "source": [
    "Slight bump in performance, but at the expense of longer training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab34f0",
   "metadata": {},
   "source": [
    "### 2.2 Not use stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50f0ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=100, ngram_range=(1,2))\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_dev_bow = vectorizer.transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6e95fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "MultinomialNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.912\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93     41413\n",
      "           1       0.84      0.91      0.88     21220\n",
      "\n",
      "    accuracy                           0.91     62633\n",
      "   macro avg       0.90      0.91      0.90     62633\n",
      "weighted avg       0.92      0.91      0.91     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[37816  3597]\n",
      " [ 1903 19317]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "BernoulliNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.828\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.87     41413\n",
      "           1       0.79      0.68      0.73     21220\n",
      "\n",
      "    accuracy                           0.83     62633\n",
      "   macro avg       0.82      0.79      0.80     62633\n",
      "weighted avg       0.83      0.83      0.82     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[37550  3863]\n",
      " [ 6884 14336]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "ComplementNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.902\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92     41413\n",
      "           1       0.80      0.95      0.87     21220\n",
      "\n",
      "    accuracy                           0.90     62633\n",
      "   macro avg       0.88      0.91      0.89     62633\n",
      "weighted avg       0.91      0.90      0.90     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[36419  4994]\n",
      " [ 1161 20059]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "Logistic Regression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.926\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94     41413\n",
      "           1       0.86      0.94      0.90     21220\n",
      "\n",
      "    accuracy                           0.93     62633\n",
      "   macro avg       0.91      0.93      0.92     62633\n",
      "weighted avg       0.93      0.93      0.93     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[38046  3367]\n",
      " [ 1289 19931]]\n",
      "Training Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = {} # A dict to store our dev set predictions\n",
    "\n",
    "for clf, name in (\n",
    "    (MultinomialNB(alpha=0.01), \"MultinomialNB\"),\n",
    "    (BernoulliNB(alpha=0.01), \"BernoulliNB\"),\n",
    "    (ComplementNB(alpha=0.1), \"ComplementNB\"),\n",
    "    (LogisticRegression(solver='lbfgs', class_weight='balanced', max_iter=1000, random_state=RANDOM_SEED), \"Logistic Regression\")\n",
    "):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Training Results - Naive Bayes & LogReg\")\n",
    "    print(name)\n",
    "    mod = train_model(clf)\n",
    "    preds[name] = mod[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a19e6",
   "metadata": {},
   "source": [
    "Performance improved, surprisingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94477f7",
   "metadata": {},
   "source": [
    "### 2.3 Set max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c9b9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=100, max_df=5000, ngram_range=(1,2))\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_dev_bow = vectorizer.transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "332660b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "MultinomialNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.911\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     41413\n",
      "           1       0.87      0.86      0.87     21220\n",
      "\n",
      "    accuracy                           0.91     62633\n",
      "   macro avg       0.90      0.90      0.90     62633\n",
      "weighted avg       0.91      0.91      0.91     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[38713  2700]\n",
      " [ 2873 18347]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "BernoulliNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.842\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.89     41413\n",
      "           1       0.82      0.68      0.75     21220\n",
      "\n",
      "    accuracy                           0.84     62633\n",
      "   macro avg       0.84      0.80      0.82     62633\n",
      "weighted avg       0.84      0.84      0.84     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[38219  3194]\n",
      " [ 6715 14505]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "ComplementNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.903\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92     41413\n",
      "           1       0.82      0.92      0.87     21220\n",
      "\n",
      "    accuracy                           0.90     62633\n",
      "   macro avg       0.89      0.91      0.90     62633\n",
      "weighted avg       0.91      0.90      0.90     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[37149  4264]\n",
      " [ 1782 19438]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "Logistic Regression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.919\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94     41413\n",
      "           1       0.86      0.91      0.88     21220\n",
      "\n",
      "    accuracy                           0.92     62633\n",
      "   macro avg       0.91      0.92      0.91     62633\n",
      "weighted avg       0.92      0.92      0.92     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[38277  3136]\n",
      " [ 1956 19264]]\n",
      "Training Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = {} # A dict to store our dev set predictions\n",
    "\n",
    "for clf, name in (\n",
    "    (MultinomialNB(alpha=0.01), \"MultinomialNB\"),\n",
    "    (BernoulliNB(alpha=0.01), \"BernoulliNB\"),\n",
    "    (ComplementNB(alpha=0.1), \"ComplementNB\"),\n",
    "    (LogisticRegression(solver='lbfgs', class_weight='balanced', max_iter=1000, random_state=RANDOM_SEED), \"Logistic Regression\")\n",
    "):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Training Results - Naive Bayes & LogReg\")\n",
    "    print(name)\n",
    "    mod = train_model(clf)\n",
    "    preds[name] = mod[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689aa697",
   "metadata": {},
   "source": [
    "Not much changes from stopwords removal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503bb7c",
   "metadata": {},
   "source": [
    "### 2.4 Clip frequency at 1\n",
    "\n",
    "Earlier, we see that there isn't any performance with BernoulliNB as compared to other methods. But what if we clip the frequency at the feature processing level? Luckily, sklearn tfidf has a `binary` parameters that allows us to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9936465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(binary=True, ngram_range=(1,1))\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_dev_bow = vectorizer.transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e08ddff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "MultinomialNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.901\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92     41413\n",
      "           1       0.83      0.89      0.86     21220\n",
      "\n",
      "    accuracy                           0.90     62633\n",
      "   macro avg       0.89      0.90      0.89     62633\n",
      "weighted avg       0.90      0.90      0.90     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[37626  3787]\n",
      " [ 2388 18832]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "BernoulliNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.823\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87     41413\n",
      "           1       0.77      0.68      0.72     21220\n",
      "\n",
      "    accuracy                           0.82     62633\n",
      "   macro avg       0.81      0.79      0.80     62633\n",
      "weighted avg       0.82      0.82      0.82     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[37021  4392]\n",
      " [ 6703 14517]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "ComplementNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.889\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91     41413\n",
      "           1       0.78      0.94      0.85     21220\n",
      "\n",
      "    accuracy                           0.89     62633\n",
      "   macro avg       0.87      0.90      0.88     62633\n",
      "weighted avg       0.90      0.89      0.89     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[35673  5740]\n",
      " [ 1209 20011]]\n",
      "Training Complete\n",
      "\n",
      "================================================================================\n",
      "Training Results - Naive Bayes & LogReg\n",
      "Logistic Regression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "accuracy:   0.921\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94     41413\n",
      "           1       0.85      0.94      0.89     21220\n",
      "\n",
      "    accuracy                           0.92     62633\n",
      "   macro avg       0.91      0.93      0.91     62633\n",
      "weighted avg       0.93      0.92      0.92     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[37816  3597]\n",
      " [ 1323 19897]]\n",
      "Training Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = {} # A dict to store our dev set predictions\n",
    "\n",
    "for clf, name in (\n",
    "    (MultinomialNB(alpha=0.01), \"MultinomialNB\"),\n",
    "    (BernoulliNB(alpha=0.01), \"BernoulliNB\"),\n",
    "    (ComplementNB(alpha=0.1), \"ComplementNB\"),\n",
    "    (LogisticRegression(solver='lbfgs', class_weight='balanced', max_iter=1000, random_state=RANDOM_SEED), \"Logistic Regression\")\n",
    "):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Training Results - Naive Bayes & LogReg\")\n",
    "    print(name)\n",
    "    mod = train_model(clf)\n",
    "    preds[name] = mod[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8d9835",
   "metadata": {},
   "source": [
    "### 2.5 Tune Logistic Regression Params\n",
    "\n",
    "So far, LogReg performs the best in terms of macro F1 score. In this section, we'll try tuning the performance of Logistic Regression, using the best Tfidf tuning result above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ec767c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=100, ngram_range=(1,2))\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_dev_bow = vectorizer.transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3bbdc5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LogReg Grid Search\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "{'C': 1.623776739188721}\n",
      "accuracy:   0.926\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94     41413\n",
      "           1       0.86      0.94      0.90     21220\n",
      "\n",
      "    accuracy                           0.93     62633\n",
      "   macro avg       0.91      0.93      0.92     62633\n",
      "weighted avg       0.93      0.93      0.93     62633\n",
      "\n",
      "confusion matrix:\n",
      "[[38078  3335]\n",
      " [ 1298 19922]]\n",
      "Training Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='lbfgs', class_weight='balanced', max_iter=3000, random_state=RANDOM_SEED)\n",
    "param_grid = {'C' : np.logspace(-4, 4, 20)}\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LogReg Grid Search\")\n",
    "clf = GridSearchCV(clf, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "best_clf = clf.fit(X_train_bow, y_train)\n",
    "print(clf.best_params_)\n",
    "y_dev_pred = clf.predict(X_dev_bow)\n",
    "score = accuracy_score(y_dev, y_dev_pred)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "print(\"classification report:\")\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_dev, y_dev_pred))\n",
    "print(\"Training Complete\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f02387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
